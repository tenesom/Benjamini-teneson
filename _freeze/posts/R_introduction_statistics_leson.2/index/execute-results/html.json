{
  "hash": "5aecfc32f7b27121bdc3d3f9f12a770a",
  "result": {
    "markdown": "---\ntitle: \"Statistics in R Programming: How to make analysis in R, by usising statistical evidance.\"\nauthor: \"Benjamini Mpinga\"\ndate: \"2023-04-20\"\ncategories: [Skills]\n---\n\n\nStatistics is a branch of Maths that deals with every aspect of the data/ Also Statistics can be refereed as a knowledge for collecting, analyzing and interpreting data. Statistics can be used to predict the future, determine the probability that a specific event will happen, or help answer questions about a survey. Statistics is used in many different fields such as business, medicine, biology, psychology and social sciences.\n\n![](control-.jpg)\n\n### STATISTICS\n\nStatistical knowledge helps to choose the proper method of collecting data and employ those samples in the correct analysis process in order to effectively produce the results. In short, statistics is a crucial process which helps to make the decision based on the data.\n\n**What is Statistics in Programming?**\n\nIn Mathematics, statistics concerns the collection of data, organisation, interpretation, analysis and data presentation.\nAn example of statistical analysis is when we have to determine the number of people in a town who watch TV out of the total population in the town. The small group of people is called the sample, which data is taken from them. \n\n\n- **descriptive, which focuses on summarizing data.**\n- **Inferential, which focuses on making conclusions about populations based on samples.**\n\nInferential Statistics – Based on the data sample taken from the population, inferential statistics makes the predictions and inferences. Both types of statistics are equally employed in the field of statistical analysis..\n\n**The characteristics of Statistics are a:**\n\n- Statistics are numerically expressed.\n- It has an aggregate of facts\n- Data are collected in systematic order\n- It should be comparable to each other\n- Data are collected for a planned purpose\n\n**The importance of statistics are...**\n\n- Statistics helps in gathering information about the appropriate quantitative data\n- It depicts the complex data in graphical form, tabular form and in diagrammatic representation to understand it easily\n- It provides the exact description and a better understanding\n- It helps in designing the effective and proper planning of the statistical inquiry in any field\n- It gives valid inferences with the reliability measures about the population parameters from the sample data\n- It helps to understand the variability pattern through the quantitative observations\n\n**Why is statistics vital?**\n\nStatistics is an important field because it helps us understand the general trends and patterns in a given data set. Statistics can be used for analyzing data and drawing conclusions from it. It can also be used for making predictions about future events and behaviors. Statistics also help us understand how things are changing over time.\n\nThe main purpose of using statistics is to plan the collected data in terms of experimental designs and statistical surveys. Statistics is considered a mathematical science that works with numerical data.\n\n**What are the uses of statistics in real life?**\n\nStatistics is an integral part of our lives. It is used in the workplace and everyday life. In the workplace, statistics are often used to analyse what works best for a company’s marketing strategy or how to distribute work among employees. In daily life, statistics can be used to analyse what food you should buy at the grocery store or how much money you spend on purchasing each week. Statistics are everywhere, and they help us make sense of the world around us.\n\n**Concept covered in Statistics.**\n\nThe most important concepts covered in Statistics include ***mean, median, mode, range, and standard deviation, Normality and Significance.*** Mean and Median in statistics usually measure the median/mean value, but their application depend on the sample's nature.\n\n### DESCRIPTIVE STATISTICS.\n\nDescriptive statistics usually used to describe type of data, by describe Median, Mean, Mode and Range in a particular data.\n\n\n1. **The mean is the average of the data set.**\n1. **The median is the middle of the set of numbers.**\n1. **The mode is the most common number in a data set.**\n\nIn these three the mean is the only one that requires formula. Mean in math is what they call Average, they are synonym.\n\n#### How to find Median. \n\nTo calculate median value when your data values are mostly alike but some few values have big difference from the other either High or Low. The purpose is to avoid the high or low values to affect the result. Basically for numeric data. \n\n**Eg. (1,2,3,4,5,6,7) to find median ....... you arrange your numbers from the lowest to the highest or from highest to the lowest then you pick the centered number. So in the set 1,2,3,4,5,6,7 the middle number is 4. \n\nBut what happens if the data set is like (1,2,3,4,5,6,7,8)?.** in that case you arrange your data set and then you add the two middle numbers and then you them divide by 2 eg 4+5 = 9/2 = 4.5.\n\nIn most cases the median and the mean are likely to have the same value, even when they differ, it's not in significance gap.\n\n#### How to find mean.\n\nThere are mainly two ways to find mean in a data set, One is by finding the sum of the values by adding them all up, dividing the sum by the number of values in the data set.\n\n**Eg. (1,2,3,4,5,6,7) to find mean ...... 1+2+3+4+5+6+7= 28/7 = 4 therefor the mean is 4. Remember the mean in a data set does not have to appear in the data set.**\n\n#### The different between Mean and Median\n\n1. The median is less affected by outliers and skewed data, is used for skewed distributions.\n1. Median is Usually the preferred measure of central tendency, when distribution is not symmetrical. \n1. The mean is usually used for normal distributed data. \n\n**Standard deviation** \n\n- You have to calculate the deviation in order to measure where your data deviate from the normal value.\n\n### INFERENTIAL STATISTICS.\n\nInferential statistics, basically focus on solution based, due to data problems, or in other word it simply true that, Data always arise either a challenge or a question, so inferential statistic is the type of statistics to solve and answer data challenge and question.\n\nAs we have seen in previous lesson, statistics plays major roll to daily life activities, therefor knowing how to apply statistics in analysis, is not only basic knowledge to acquire but also essential tool to approach and solve some analytic data based problems.\n\nSo in this lesson we shall focus on the second type of statistics, which often used to transform and analyze and give analytic results which is what drive decision making. Inferential statistics help in solving many problems which require analytics approach, and scientific based evidence. So if you want to become a data analyst, researcher, then this angle is where you should focus attentively. We shall highlight the tips used to approach the question, and theory to solve analytics problems.\n\n### DATA COLLECTION THEORY.\n\nIn sample collection, it is advise to collect at least 10 to 30 percent samples of general population, this is to ensure samples can provide accurate results. This is due to fact that, the less ample you collect the less efficient result you may get, also the more sample you collect might be unnecessary, this means it might not make any change in results. So it is advice to collect a reasonable sample for wise utilization of resources as well as time management but also to get efficient results.\n\nOften data are collected by researcher(s), who focus on a specific matter or subject, So there some steps to consider before going field. One is to set your project plan, two is to give project a tile, three is to set objectives then finally to rise hypothesis of the project. Now in this lesson shall focus in how to start and conclude the results of your research.\n\n### HYPOTHESIS THEORY.\n\nOne of the sediment in an inferential statistics, is called Hypothesis, which means all the assumption rouse by a researcher or annalist before data collections, usually before data collections there are some basic ground considered to be true or false. those reasonable grounds considered to be true or false are what called Hypothesis (Assumptions). Hypothesis are divided in two types.\n\n1.  **Null Hypothesis -- Ho -- Negative.**\n2.  **Alternative Hypothesis -- Ha -- Positive** \n\nIn statistic analysis we use different approach to address solutions to our problems, but most often our approach leads us to either solve or cause more trouble to the problem. In standard approach of statistic data analysis, it is required to approach the data results, by ensuring both the significance and Insignificance of the results, as well as normality of the data. In this step the approach to use is Null Hypothesis theory.\n\n### TEST SIGNIFICANCE.\n\n1. Null hypothesis theory **(NO SIGNIFICANT DIFFERENCE).**\n\nNull hypothesis theory, is usually used to conclude grounds of the research, so every standard research relay on this theory. As i have stated before, in any research there must be some assumptions fore to sample collection. So to make conclusion of your results, to make conclusion we relay on significance level, significant level is measured by P value, so when the **P value = > 0.05 then it's considered There is No significant difference. And when P value = < 0.05 then it's considered There is significant difference.**\n\n1. Alternative Hypothesis Theory. **(THERE IS SIGNIFICANT DIFFERENT).**\n\nAlternative Hypothesis is sometimes used by some statisticians to approach some problems in data, but it's not recommended in professional conduct, it doesn't mean this theory has anything wrong, but simply because in concluding our findings in data, we relay on Null Hypothesis. therefor Alternative Hypothesis  remains as an opposite of Null Hypothesis.\n\n### TEST NORMALITY FORMULA.\n\nNormality refers to the distribution of your collected data, this means after you have describe your data in descriptive type of statistics, then you have to test them, if their normal distributed or not normal distributed. In normality formula we  prescribe other way round, which means, **(Null Hypothesis - Ho = Is normal distributed while Alternative Hypothesis - Ha = Is not normal distributed.)**\n\n**Normality formula.**\n\n- When P value = > 0.05 then you Accept the Null. \n- When P value = < 0.05 then you Reject the Null.\n\n### TEST NORMALITY IN R.\n\nIn R, we test the distributions of the data by using the function (shapiro), the below example is how you can test the distribution of the data in R analysis. The data we are using is auto generated from R data base. In R you can find some data to help you improve your analysis practice, in case you don't have access to acquire data from other source. to generate data in R, just create a chunk then write iris then run the chunk, and data will appear on your R editor. After that you can equate them to another name.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrequire(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: tidyverse\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.0     v readr     2.1.4\nv forcats   1.0.0     v stringr   1.5.0\nv ggplot2   3.4.1     v tibble    3.1.8\nv lubridate 1.9.2     v tidyr     1.3.0\nv purrr     1.0.1     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsample.iris=iris\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsample.iris$Sepal.Length %>%  shapiro.test()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  .\nW = 0.97609, p-value = 0.01018\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = sample.iris, aes(x = Sepal.Length))+\n  geom_density(fill = \"darkred\",alpha = 0.3)+\n  theme_bw()+\n   theme(panel.grid.major.x = element_line(color = \"lightgrey\", linetype = \"dashed\"), panel.grid.major.y = element_line(color = \"lightgrey\", linetype = \"dashed\"), panel.grid = element_line(color = \"white\"), axis.title = element_text(color = \"darkred\", size = 12), axis.text = element_text(color = \"darkred\"), axis.line.x = element_line(linewidth = 1), axis.line.y = element_line(linewidth = 1))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\nwe have tested Sepal.L data normality in R, and results shows that they are not normal distributed. Then we have tried to plot in order to observe visual distribution, and results still shows that Sepal.Length are not normal distributed. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample.iris$Sepal.Width %>% shapiro.test()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  .\nW = 0.98492, p-value = 0.1012\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = sample.iris, aes(x = Sepal.Width))+\n  geom_density(fill=\"darkred\",alpha = 0.3)+\n  theme_bw()+\n  theme(panel.grid.major.x = element_line(color = \"lightgrey\", linetype = \"dashed\"), panel.grid.major.y = element_line(color = \"lightgrey\", linetype = \"dashed\"), panel.grid = element_line(color = \"white\"), axis.title = element_text(color = \"darkred\", size = 12), axis.text = element_text(color = \"darkred\"), axis.line.x = element_line(linewidth = 1), axis.line.y = element_line(linewidth = 1))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\nwe have tested Sepal.Width data normality in R, and results shows that they are normal distributed. Then we have tried to plot in order to observe visual distribution, and results still shows that Sepal.Width are normal distributed. \n\nThis is how we test data in R programming language, it is easier and clear as we have seen. S o working your data in R, is not only a recommended tool but also facilitate mass data solutions. \n\n\n### CENTRAL LIMIT THEORY (CLT)\n\nIn statistics the central limit theory means the value on which majority of population falls. It simply states that The distribution approaches normal, as the samples get larger. therefor it is recommended that to attain doubtless results you are required to collect at least >/= of 30% samples of the population.\n\nThis theory of central limit usually shown through continuous plots such as Histogram and density. This is due to their nature. On the figure below we are going to see the example of central limit theory in visualization.\n\nIn this study we will see how statistics works in real life, and how useful it is in every day's decision making. By applying a proper and correct method in collecting/recording samples, analyzing, interpreting and sharing of findings is always a best way to reach the right decisions, especially for decision makers, weather in family level.\n\n#### What is the purpose of dispersion\n\nThe purpose is to find out how spread out the data values are, on the number line, the deference between the maximum and the minimum value is the formula to measure dispersion (Measure of spread).\n\n#### What does it mean by Measure of central tendency\n\nSimply is defined as A measure that identifies a single value as representative of the entire distribution. It is the single value that is most typical/representative of the collected data.\n\n**Related pots**",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}